import streamlit as st
import requests
import feedparser
import pandas as pd
from datetime import datetime
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document
from bs4 import BeautifulSoup

st.set_page_config(page_title="ðŸ“° Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ðŸ—žï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (RSS + Web Scraping)")

category_keywords = {
    "Ø³ÙŠØ§Ø³Ø©": ["\u0631\u0626\u064a\u0633", "\u0648\u0632\u064a\u0631", "\u0627\u0646\u062a\u062e\u0627\u0628\u0627\u062a", "\u0628\u0631\u0644\u0645\u0627\u0646", "\u0633\u064a\u0627\u0633\u0629"],
    "Ø±ÙŠØ§Ø¶Ø©": ["\u0643\u0631\u0629", "\u0644\u0627\u0639\u0628", "\u0645\u0628\u0627\u0631\u0627\u0629", "\u062f\u0648\u0631\u064a", "\u0647\u062f\u0641"],
    "Ø§Ù‚ØªØµØ§Ø¯": ["\u0633\u0648\u0642", "\u0627\u0642\u062a\u0635\u0627\u062f", "\u0627\u0633\u062a\u062b\u0645\u0627\u0631", "\u0628\u0646\u0643", "\u0645\u0627\u0644"],
    "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["\u062a\u0642\u0646\u064a\u0629", "\u062a\u0637\u0628\u064a\u0642", "\u0647\u0627\u062a\u0641", "\u0630\u0643\u0627\u0621", "\u0628\u0631\u0645\u062c\u0629"]
}

def summarize(text, max_words=25):
    return " ".join(text.split()[:max_words]) + "..."

def analyze_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return "ðŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif polarity < -0.1:
        return "ðŸ˜  Ø³Ù„Ø¨ÙŠ"
    else:
        return "ðŸ˜ Ù…Ø­Ø§ÙŠØ¯"

def detect_category(text):
    for category, words in category_keywords.items():
        if any(word in text for word in words):
            return category
    return "ØºÙŠØ± Ù…ØµÙ†Ù‘Ù"

def fetch_scraped_news(url, source_name, keywords, date_from, date_to, chosen_category, selector, link_prefix=""):
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "html.parser")
    news_list = []

    for a in soup.select(selector):
        title = a.text.strip()
        link = a.get("href", "")
        if link_prefix and not link.startswith("http"):
            link = link_prefix + link
        published_dt = datetime.today()

        if keywords and not any(k.lower() in title.lower() for k in keywords):
            continue

        auto_category = detect_category(title)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": source_name,
            "title": title,
            "summary": title,
            "link": link,
            "published": published_dt,
            "image": "",
            "sentiment": analyze_sentiment(title),
            "category": auto_category
        })

    return news_list

def fetch_news(source_name, source_data, keywords, date_from, date_to, chosen_category):
    if source_data["type"] == "rss":
        feed = feedparser.parse(source_data["url"])
        news_list = []
        for entry in feed.entries:
            title = entry.title
            summary = entry.get("summary", "")
            link = entry.link
            published = entry.get("published", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            try:
                published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
            except:
                published_dt = datetime.now()
            image = ""
            if 'media_content' in entry:
                image = entry.media_content[0].get('url', '')
            elif 'media_thumbnail' in entry:
                image = entry.media_thumbnail[0].get('url', '')

            if not (date_from <= published_dt.date() <= date_to):
                continue

            full_text = title + " " + summary
            if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                continue

            auto_category = detect_category(full_text)
            if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
                continue

            news_list.append({
                "source": source_name,
                "title": title,
                "summary": summary,
                "link": link,
                "published": published_dt,
                "image": image,
                "sentiment": analyze_sentiment(summary),
                "category": auto_category
            })
        return news_list

    elif source_data["type"] == "scrape":
        return fetch_scraped_news(source_data["url"], source_name, keywords, date_from, date_to, chosen_category, source_data["selector"], source_data.get("prefix", ""))

# Ø§Ù„Ù…ØµØ§Ø¯Ø±
rss_feeds = {
    "RT Arabic": {"type": "rss", "url": "https://arabic.rt.com/rss/"},
    "Hatha Alyoum": {"type": "scrape", "url": "https://hathalyoum.net/", "selector": ".newstitle a"},
    "Iraq Today": {"type": "scrape", "url": "https://iraqtoday.com/", "selector": ".block-title a"},
    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©": {"type": "scrape", "url": "https://moi.gov.iq/", "selector": ".more-news .title-news a", "prefix": "https://moi.gov.iq"},
    "Ø±Ø¦Ø§Ø³Ø© Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ©": {"type": "scrape", "url": "https://presidency.iq/", "selector": ".article-title a"},
    "Ø§Ù„Ø´Ø±Ù‚ - Ø§Ù„Ø¹Ø±Ø§Ù‚": {"type": "scrape", "url": "https://asharq.com/tags/Ø§Ù„Ø¹Ø±Ø§Ù‚/", "selector": "a.Card", "prefix": "https://asharq.com"},
    "RT - Ø§Ù„Ø¹Ø±Ø§Ù‚": {"type": "scrape", "url": "https://arabic.rt.com/focuses/10744-Ø§Ù„Ø¹Ø±Ø§Ù‚/", "selector": ".focus-page__item a.focus-page__link", "prefix": "https://arabic.rt.com"},
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - Ø§Ù„Ø¹Ø±Ø§Ù‚": {"type": "scrape", "url": "https://x.com/AlArabiya_Iraq?t=_o4RgF2gn5IEz3a8WZQ_GQ&s=09", "selector": "title"}  # placeholder
}

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­ÙƒÙ…
col1, col2 = st.columns([1, 2])
with col1:
    selected_source = st.selectbox("ðŸŒ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:", list(rss_feeds.keys()))
    keywords_input = st.text_input("ðŸ” ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„):", "")
    keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []
    category_filter = st.selectbox("ðŸ“ Ø§Ø®ØªØ± Ø§Ù„ØªØµÙ†ÙŠÙ:", ["Ø§Ù„ÙƒÙ„"] + list(category_keywords.keys()))
    date_from = st.date_input("ðŸ—“ Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today())
    date_to = st.date_input("ðŸ—“ Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())
    run = st.button("ðŸ“… Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")

with col2:
    if run:
        news = fetch_news(
            selected_source,
            rss_feeds[selected_source],
            keywords,
            date_from,
            date_to,
            category_filter
        )

        if not news:
            st.warning("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙˆØ·.")
        else:
            st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news)} Ø®Ø¨Ø±.")
            for item in news:
                with st.container():
                    st.markdown("----")
                    cols = st.columns([1, 4])
                    with cols[0]:
                        if item["image"]:
                            st.image(item["image"], use_column_width=True)
                    with cols[1]:
                        st.markdown(f"### ðŸ“° {item['title']}")
                        st.markdown(f"ðŸ—“ Ø§Ù„ØªØ§Ø±ÙŠØ®: {item['published'].strftime('%Y-%m-%d')}")
                        st.markdown(f"ðŸ“ Ø§Ù„ØªØµÙ†ÙŠÙ: {item['category']}")
                        st.markdown(f"ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(item['summary'])}")
                        st.markdown(f"ðŸŽ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {item['sentiment']}")
                        st.markdown(f"[ðŸŒ Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯ â†—]({item['link']})")

            def export_to_word(news_list):
                doc = Document()
                for news in news_list:
                    doc.add_heading(news['title'], level=2)
                    doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}  |  Ø§Ù„ØªØµÙ†ÙŠÙ: {news['category']}")
                    doc.add_paragraph(f"ðŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
                    doc.add_paragraph(f"ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(news['summary'])}")
                    doc.add_paragraph(f"ðŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
                    doc.add_paragraph(f"ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù†ÙˆÙŠ: {news['sentiment']}")
                    doc.add_paragraph("-----")
                buffer = BytesIO()
                doc.save(buffer)
                buffer.seek(0)
                return buffer

            def export_to_excel(news_list):
                df = pd.DataFrame(news_list)
                buffer = BytesIO()
                with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False)
                buffer.seek(0)
                return buffer

            st.download_button("ðŸ“„ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Word", data=export_to_word(news), file_name="news.docx",
                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
            st.download_button("ðŸ“Š ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Excel", data=export_to_excel(news), file_name="news.xlsx",
                               mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

            st.markdown("### ðŸ”  Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ù‹Ø§:")
            all_text = " ".join([n['summary'] for n in news])
            words = [word for word in all_text.split() if len(word) > 3]
            word_freq = Counter(words).most_common(10)
            for word, freq in word_freq:
                st.markdown(f"- **{word}**: {freq} Ù…Ø±Ø©")
