import streamlit as st
import feedparser
import requests
import pandas as pd
from datetime import datetime
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document
from bs4 import BeautifulSoup

st.set_page_config(page_title="ðŸ“° Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ðŸ—žï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© + Ø¯Ø¹Ù… Ù…ÙˆØ§Ù‚Ø¹ Ø¹Ø±Ø§Ù‚ÙŠØ©)")

# Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
category_keywords = {
    "Ø³ÙŠØ§Ø³Ø©": ["Ø±Ø¦ÙŠØ³", "ÙˆØ²ÙŠØ±", "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª", "Ø¨Ø±Ù„Ù…Ø§Ù†", "Ø³ÙŠØ§Ø³Ø©", "Ø±Ø¦Ø§Ø³Ø©"],
    "Ø±ÙŠØ§Ø¶Ø©": ["ÙƒØ±Ø©", "Ù„Ø§Ø¹Ø¨", "Ù…Ø¨Ø§Ø±Ø§Ø©", "Ø¯ÙˆØ±ÙŠ", "Ù‡Ø¯Ù"],
    "Ø§Ù‚ØªØµØ§Ø¯": ["Ø³ÙˆÙ‚", "Ø§Ù‚ØªØµØ§Ø¯", "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø¨Ù†Ùƒ", "Ù…Ø§Ù„"],
    "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["ØªÙ‚Ù†ÙŠØ©", "ØªØ·Ø¨ÙŠÙ‚", "Ù‡Ø§ØªÙ", "Ø°ÙƒØ§Ø¡", "Ø¨Ø±Ù…Ø¬Ø©"]
}

# --- Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªØµÙ†ÙŠÙ ---
def summarize(text, max_words=25):
    return " ".join(text.split()[:max_words]) + "..."

def analyze_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return "ðŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif polarity < -0.1:
        return "ðŸ˜  Ø³Ù„Ø¨ÙŠ"
    else:
        return "ðŸ˜ Ù…Ø­Ø§ÙŠØ¯"

def detect_category(text):
    for category, words in category_keywords.items():
        if any(word in text for word in words):
            return category
    return "ØºÙŠØ± Ù…ØµÙ†Ù‘Ù"

# --- Scrapers ---
def fetch_hathalyoum_news(keywords, date_from, date_to, chosen_category):
    url = "https://hathalyoum.net/"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "html.parser")
    news_list = []

    for article in soup.select(".newstitle a"):
        title = article.text.strip()
        link = article["href"]
        summary = title
        published_dt = datetime.today()

        full_text = title
        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
            continue
        auto_category = detect_category(full_text)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": "Ù‡ÙŽØ°Ø§ Ø§Ù„ÙŠÙˆÙ…",
            "title": title,
            "summary": summary,
            "link": link,
            "published": published_dt,
            "image": "",
            "sentiment": analyze_sentiment(summary),
            "category": auto_category
        })
    return news_list

def fetch_iraqtoday_news(keywords, date_from, date_to, chosen_category):
    url = "https://iraqtoday.com/"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "html.parser")
    news_list = []

    for article in soup.select(".block-title a"):
        title = article.text.strip()
        link = article["href"]
        summary = title
        published_dt = datetime.today()

        full_text = title
        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
            continue
        auto_category = detect_category(full_text)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": "Iraq Today",
            "title": title,
            "summary": summary,
            "link": link,
            "published": published_dt,
            "image": "",
            "sentiment": analyze_sentiment(summary),
            "category": auto_category
        })
    return news_list

def fetch_moi_news(keywords, date_from, date_to, chosen_category):
    url = "https://moi.gov.iq/"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "html.parser")
    news_list = []

    for article in soup.select(".more-news .title-news a"):
        title = article.text.strip()
        link = article["href"]
        if not link.startswith("http"):
            link = "https://moi.gov.iq" + link
        summary = title
        published_dt = datetime.today()

        full_text = title
        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
            continue
        auto_category = detect_category(full_text)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©",
            "title": title,
            "summary": summary,
            "link": link,
            "published": published_dt,
            "image": "",
            "sentiment": analyze_sentiment(summary),
            "category": auto_category
        })
    return news_list

def fetch_presidency_news(keywords, date_from, date_to, chosen_category):
    url = "https://presidency.iq/"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "html.parser")
    news_list = []

    for article in soup.select(".article-title a"):
        title = article.text.strip()
        link = article["href"]
        summary = title
        published_dt = datetime.today()

        full_text = title
        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
            continue
        auto_category = detect_category(full_text)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": "Ø±Ø¦Ø§Ø³Ø© Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ©",
            "title": title,
            "summary": summary,
            "link": link,
            "published": published_dt,
            "image": "",
            "sentiment": analyze_sentiment(summary),
            "category": auto_category
        })
    return news_list

def fetch_asharq_iraq_news(keywords, date_from, date_to, chosen_category):
    url = "https://asharq.com/tags/%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "html.parser")
    news_list = []

    for article in soup.select("a.Card"):
        title = article.text.strip()
        link = "https://asharq.com" + article["href"]
        summary = title
        published_dt = datetime.today()

        full_text = title
        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
            continue
        auto_category = detect_category(full_text)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": "Ø§Ù„Ø´Ø±Ù‚ - Ø§Ù„Ø¹Ø±Ø§Ù‚",
            "title": title,
            "summary": summary,
            "link": link,
            "published": published_dt,
            "image": "",
            "sentiment": analyze_sentiment(summary),
            "category": auto_category
        })
    return news_list

def fetch_rt_iraq_news(keywords, date_from, date_to, chosen_category):
    url = "https://arabic.rt.com/focuses/10744-Ø§Ù„Ø¹Ø±Ø§Ù‚/"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "html.parser")
    news_list = []

    for article in soup.select(".news-line a"):
        title = article.text.strip()
        link = "https://arabic.rt.com" + article["href"]
        summary = title
        published_dt = datetime.today()

        full_text = title
        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
            continue
        auto_category = detect_category(full_text)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": "RT - Ø§Ù„Ø¹Ø±Ø§Ù‚",
            "title": title,
            "summary": summary,
            "link": link,
            "published": published_dt,
            "image": "",
            "sentiment": analyze_sentiment(summary),
            "category": auto_category
        })
    return news_list

# --- Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ---
rss_feeds = {
    "Ù‡ÙŽØ°Ø§ Ø§Ù„ÙŠÙˆÙ…": {"type": "scrape", "function": fetch_hathalyoum_news},
    "Iraq Today": {"type": "scrape", "function": fetch_iraqtoday_news},
    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©": {"type": "scrape", "function": fetch_moi_news},
    "Ø±Ø¦Ø§Ø³Ø© Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ©": {"type": "scrape", "function": fetch_presidency_news},
    "Ø§Ù„Ø´Ø±Ù‚ - Ø§Ù„Ø¹Ø±Ø§Ù‚": {"type": "scrape", "function": fetch_asharq_iraq_news},
    "RT - Ø§Ù„Ø¹Ø±Ø§Ù‚": {"type": "scrape", "function": fetch_rt_iraq_news}
}

# --- ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­ÙƒÙ… ---
col1, col2 = st.columns([1, 2])
with col1:
    selected_source = st.selectbox("ðŸŒ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:", list(rss_feeds.keys()))
    keywords_input = st.text_input("ðŸ” ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„):", "")
    keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []
    category_filter = st.selectbox("ðŸ“ Ø§Ø®ØªØ± Ø§Ù„ØªØµÙ†ÙŠÙ:", ["Ø§Ù„ÙƒÙ„"] + list(category_keywords.keys()))
    date_from = st.date_input("ðŸ“… Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today())
    date_to = st.date_input("ðŸ“… Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())
    run = st.button("ðŸ“¥ Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")

# --- ØªØ­Ù„ÙŠÙ„ ÙˆÙ†ØªØ§Ø¦Ø¬ ---
def export_to_word(news_list):
    doc = Document()
    for news in news_list:
        doc.add_heading(news['title'], level=2)
        doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}  |  Ø§Ù„ØªØµÙ†ÙŠÙ: {news['category']}")
        doc.add_paragraph(f"ðŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(news['summary'])}")
        doc.add_paragraph(f"ðŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
        doc.add_paragraph(f"ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù†ÙˆÙŠ: {news['sentiment']}")
        doc.add_paragraph("-----")
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    buffer.seek(0)
    return buffer

with col2:
    if run:
        source_data = rss_feeds[selected_source]
        news = source_data["function"](keywords, date_from, date_to, category_filter)

        if not news:
            st.warning("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙˆØ·.")
        else:
            st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news)} Ø®Ø¨Ø±.")
            for item in news:
                with st.container():
                    st.markdown("----")
                    cols = st.columns([1, 4])
                    with cols[0]:
                        if item["image"]:
                            st.image(item["image"], use_column_width=True)
                    with cols[1]:
                        st.markdown(f"### ðŸ“° {item['title']}")
                        st.markdown(f"ðŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {item['published'].strftime('%Y-%m-%d')}")
                        st.markdown(f"ðŸ“ Ø§Ù„ØªØµÙ†ÙŠÙ: {item['category']}")
                        st.markdown(f"ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(item['summary'])}")
                        st.markdown(f"ðŸŽ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {item['sentiment']}")
                        st.markdown(f"[ðŸŒ Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯ â†—]({item['link']})")

            st.download_button("ðŸ“„ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Word", data=export_to_word(news), file_name="news.docx",
                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
            st.download_button("ðŸ“Š ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Excel", data=export_to_excel(news), file_name="news.xlsx",
                               mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

            st.markdown("### ðŸ”  Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ù‹Ø§:")
            all_text = " ".join([n['summary'] for n in news])
            words = [word for word in all_text.split() if len(word) > 3]
            word_freq = Counter(words).most_common(10)
            for word, freq in word_freq:
                st.markdown(f"- **{word}**: {freq} Ù…Ø±Ø©")
